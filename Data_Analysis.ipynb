{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "import six\n",
    "import math\n",
    "import time\n",
    "import Quandl\n",
    "import calendar\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import pylab as pylab\n",
    "from docx import Document\n",
    "from datetime import datetime\n",
    "\n",
    "from matplotlib import colors\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "import statsmodels.stats.stattools as stats_stattools\n",
    "import statsmodels.tsa.stattools as tsa_stattools\n",
    "import statsmodels.tsa.seasonal as tsa_seasonal\n",
    "import statsmodels.api as sm \n",
    "import xgboost as xgb\n",
    "from unbalanced_dataset import SMOTE\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn import metrics, cross_validation, linear_model, naive_bayes, neighbors, ensemble\n",
    "from sklearn import feature_selection\n",
    "from sklearn import decomposition\n",
    "from sklearn import discriminant_analysis\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import sys\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from helpers import features_analysis, procces_stocks, data_manipulation, download_quandl_data, ml_dataset, classifier_utils, report_generator, Iteration, Stacking, Boosting\n",
    "\n",
    "fig_size = [10, 6]\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "sb.set_style('darkgrid')\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GOLD = data_manipulation.read_csv_data('/Users/Pablo/Desktop/TFM/Data/GOLD.csv')\n",
    "SILVER = data_manipulation.read_csv_data('/Users/Pablo/Desktop/TFM/Data/SILVER.csv')\n",
    "PLAT = data_manipulation.read_csv_data('/Users/Pablo/Desktop/TFM/Data/PLAT.csv')\n",
    "OIL_BRENT = data_manipulation.read_csv_data('/Users/Pablo/Desktop/TFM/Data/OIL_BRENT.csv')\n",
    "\n",
    "USD_GBP = data_manipulation.read_csv_data('/Users/Pablo/Desktop/TFM/Data/USD_GBP.csv')\n",
    "JPY_USD = data_manipulation.read_csv_data('/Users/Pablo/Desktop/TFM/Data/JPY_USD.csv')\n",
    "AUD_USD = data_manipulation.read_csv_data('/Users/Pablo/Desktop/TFM/Data/AUD_USD.csv')\n",
    "\n",
    "INDEX_DJIA = data_manipulation.read_csv_data('/Users/Pablo/Desktop/TFM/Data/INDEX_DJIA.csv')\n",
    "INDEX_HSI = data_manipulation.read_csv_data('/Users/Pablo/Desktop/TFM/Data/INDEX_HSI.csv')\n",
    "INDEX_IBEX = data_manipulation.read_csv_data('/Users/Pablo/Desktop/TFM/Data/INDEX_IBEX.csv')\n",
    "INDEX_N225 = data_manipulation.read_csv_data('/Users/Pablo/Desktop/TFM/Data/INDEX_N225.csv')\n",
    "INDEX_SP500 = data_manipulation.read_csv_data('/Users/Pablo/Desktop/TFM/Data/INDEX_SP500.csv')\n",
    "INDEX_AXJO = data_manipulation.read_csv_data('/Users/Pablo/Desktop/TFM/Data/INDEX_AXJO.csv')\n",
    "INDEX_FCHI = data_manipulation.read_csv_data('/Users/Pablo/Desktop/TFM/Data/INDEX_FCHI.csv')\n",
    "INDEX_GDAXI = data_manipulation.read_csv_data('/Users/Pablo/Desktop/TFM/Data/INDEX_GDAXI.csv')\n",
    "\n",
    "values_names = ['GOLD', 'SILVER', 'PLAT', 'OIL_BRENT', 'USD_GBP', 'JPY_USD', 'AUD_USD', 'DJIA', 'HSI', 'IBEX', 'N225', 'SP500', 'AXJO', 'FCHI', 'GDAXI']\n",
    "values_dfs = [GOLD, SILVER, PLAT, OIL_BRENT, USD_GBP, JPY_USD, AUD_USD, INDEX_DJIA, INDEX_HSI, INDEX_IBEX, INDEX_N225, INDEX_SP500, INDEX_AXJO, INDEX_FCHI, INDEX_GDAXI]\n",
    "values_cols = ['USD', 'Value', 'Open', 'Close', 'High', 'Low', 'Volume']\n",
    "dict_dfs_cols = {}\n",
    "dict_dfs_cols2 = {}\n",
    "\n",
    "for index in range(len(values_names)):\n",
    "    name = values_names[index]\n",
    "    df = values_dfs[index]    \n",
    "    cols = df.columns.values\n",
    "\n",
    "    new_cols = [x for x in cols if x not in ['Date', 'USD', 'Value', 'Open', 'Close', 'High', 'Low', 'Volume']]    \n",
    "    new_cols2 = [x for x in cols if x not in ['Date']]    \n",
    "\n",
    "    dict_dfs_cols[name] = new_cols\n",
    "    dict_dfs_cols2[name] = new_cols2\n",
    "\n",
    "dataset = ml_dataset.generate_df_dataset(values_names, values_dfs, dict_dfs_cols)\n",
    "dataset_all = ml_dataset.generate_df_dataset(values_names, values_dfs, dict_dfs_cols2)\n",
    "\n",
    "\n",
    "\n",
    "#First 30 row\n",
    "dataset = dataset[31:]\n",
    "dataset = dataset.reset_index(drop=True)\n",
    "#colsToShift = [col for col in dataset.columns if 'HSI' in col or'N225' in col or'AXJO' in col]\n",
    "#dataset[colsToShift] = dataset[colsToShift].shift(-1)\n",
    "#last_row = dataset.shape[0]-1\n",
    "#dataset = dataset.drop(last_row, axis=0)  \n",
    "\n",
    "dataset_all = dataset_all[31:]\n",
    "dataset_all = dataset_all.reset_index(drop=True)\n",
    "#dataset_all[colsToShift] = dataset_all[colsToShift].shift(-1)\n",
    "#last_row = dataset_all.shape[0]-1\n",
    "#dataset_all = dataset_all.drop(last_row, axis=0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using params LR:all_greedy: {'penalty': u'l2', 'C': 1.5, 'random_state': 42}\n",
      "using params SGDC:all_greedy: {'loss': 'log'}\n",
      "using params GNB:all_greedy: {}\n",
      "using params BNB:all_greedy: {u'alpha': 0.0, u'fit_prior': True}\n",
      "using params KNC:all_greedy: {u'n_neighbors': 10, u'weights': u'distance', u'algorithm': u'ball_tree', u'p': 3}\n",
      "using params ABC:all_greedy: {u'n_estimators': 5, u'learning_rate': 0.08, 'random_state': 42}\n",
      "using params BC:all_greedy: {u'n_estimators': 10, 'random_state': 42, 'n_jobs': -1}\n",
      "using params GBC:all_greedy: {'learning_rate': 0.05, 'min_samples_leaf': 1, 'min_samples_split': 3, 'random_state': 42, 'max_features': 4, 'max_depth': 13}\n",
      "using params RFC:all_greedy: {'n_jobs': -1, 'min_samples_leaf': 2, 'n_estimators': 500, 'max_features': 3, 'bootstrap': False, 'random_state': 42, 'min_samples_split': 7, 'max_depth': 30}\n",
      "using params ETC:all_greedy: {'n_jobs': -1, 'bootstrap': False, 'min_samples_leaf': 3, 'n_estimators': 500, 'max_features': 1, 'min_samples_split': 5, 'max_depth': 20}\n"
     ]
    }
   ],
   "source": [
    "training_dates = Iteration.Iteration('2009-08-19', '2014-12-01')\n",
    "testing_dates  = Iteration.Iteration('2014-12-02', '2016-04-20')\n",
    "training_dates.calculate_indices(dataset)\n",
    "testing_dates.calculate_indices(dataset)\n",
    "\n",
    "trainDates = []\n",
    "testDates = []\n",
    "trainDates.append(training_dates.lowerIndex)\n",
    "trainDates.append(training_dates.upperIndex)\n",
    "testDates.append(testing_dates.lowerIndex)\n",
    "testDates.append(testing_dates.upperIndex)\n",
    "\n",
    "trainX, trainY, testX, testY, cols = ml_dataset.dataset_to_train_using_dates(dataset, trainDates, testDates, binary=False, shiftFeatures=False, shiftTarget=False)\n",
    "\n",
    "N_TREES = 500\n",
    "SEED = 42\n",
    "log = open(\"log.txt\", \"w\")\n",
    "\n",
    "selected_models = [\n",
    "    \"LRC:all_greedy\",   \n",
    "    \"SGD:all_greedy\",\n",
    "\n",
    "#    \"LDA:all_greedy\",\n",
    "#    \"QDA:all_greedy\",\n",
    "#   \n",
    "#    \"SVM:all_greedy\",\n",
    "#   \n",
    "    \"NBG:all_greedy\",\n",
    "    \"NBB:all_greedy\",\n",
    "\n",
    "    \"KNN:all_greedy\", \n",
    "    \n",
    "    \"ABC:all_greedy\",\n",
    "    \"BGC:all_greedy\",\n",
    "    \"GBC:all_greedy\", \n",
    "    \"RFC:all_greedy\",\n",
    "    \"ETC:all_greedy\"\n",
    "]\n",
    "\n",
    "algorithms_list = ['']\n",
    "\n",
    "# Create the models on the fly\n",
    "models = []\n",
    "for item in selected_models:\n",
    "    model_id, data_set = item.split(':')\n",
    "    model = {'LRC':linear_model.LogisticRegression,\n",
    "             'SGD':linear_model.SGDClassifier,\n",
    "             \n",
    "             'LDA':discriminant_analysis.LinearDiscriminantAnalysis,\n",
    "             'QDA':discriminant_analysis.QuadraticDiscriminantAnalysis,\n",
    "             \n",
    "             'SVM':svm.SVC,\n",
    "            \n",
    "             'NBG':naive_bayes.GaussianNB,\n",
    "             'NBB':naive_bayes.BernoulliNB,\n",
    "             \n",
    "             \"KNN\":neighbors.KNeighborsClassifier,\n",
    "\n",
    "             'ABC': ensemble.AdaBoostClassifier,\n",
    "             'BGC': ensemble.BaggingClassifier,             \n",
    "             'GBC': ensemble.GradientBoostingClassifier,\n",
    "             'RFC': ensemble.RandomForestClassifier,\n",
    "             'ETC': ensemble.ExtraTreesClassifier        \n",
    "            }[model_id]()\n",
    "    models.append((model, data_set))\n",
    "    algorithms_list.append(model_id)\n",
    "\n",
    "\n",
    "grid_search = True\n",
    "## Set params\n",
    "for model, feature_set in models:\n",
    "    model.set_params(**classifier_utils.find_params(model, feature_set, trainX, trainY, grid_search))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "                    Dates 2                     \n",
      "==================================================\n",
      "--------------------------------------------------\n",
      "                   Iteration 1           \n",
      "--------------------------------------------------\n",
      "..................................................\n",
      "                 Testing array 0           \n",
      "..................................................\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'trainX' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-47d9c803e850>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0;32mprint\u001b[0m \u001b[0;34m>>\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"%d training %d testing\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrainDates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtrainDates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestDates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtestDates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0mtrainX_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestX_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mml_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_train_arrays_experiments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_np\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_np\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainDates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestDates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m             \u001b[0;32mprint\u001b[0m \u001b[0mtrainX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;32mprint\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Pablo/Desktop/TFM/helpers/ml_dataset.py\u001b[0m in \u001b[0;36mnp_train_arrays_experiments\u001b[0;34m(df_x, df_y, trainDates, testDates)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0mtest_y\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mdf_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtestDates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtestDates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'trainX' is not defined"
     ]
    }
   ],
   "source": [
    "dates = {#'Dates 1': [\n",
    "         #{'training': ['1993-08-19', '2011-07-08'], 'testing': ['2011-07-11', '2016-04-20']},\n",
    "         #{'training': ['1993-08-19', '2012-07-06'], 'testing': ['2012-07-09', '2016-04-20']},\n",
    "         #{'training': ['1993-08-19', '2013-07-08'], 'testing': ['2013-07-09', '2016-04-20']}]\n",
    "         'Dates 2': [\n",
    "          {'training': ['1993-08-19', '2000-08-18'], 'testing': ['2000-08-21', '2000-09-21', '2000-08-21', '2001-08-20']},\n",
    "          {'training': ['1995-08-18', '2002-08-19'], 'testing': ['2002-08-20', '2002-09-20', '2002-08-20', '2003-08-20']},\n",
    "#          {'training': ['1997-08-19', '2004-08-19'], 'testing': ['2004-08-20', '2004-09-20', '2004-08-20', '2005-08-19']},\n",
    "          {'training': ['1999-08-19', '2006-08-18'], 'testing': ['2006-08-21', '2006-09-20', '2006-08-21', '2007-08-20']},\n",
    "#          {'training': ['2001-08-17', '2008-08-19'], 'testing': ['2008-08-20', '2008-09-22', '2008-08-20', '2009-08-20']},\n",
    "#          {'training': ['2003-08-19', '2010-08-19'], 'testing': ['2010-08-19', '2010-09-20', '2010-08-19', '2011-08-19']},\n",
    "          {'training': ['2005-08-19', '2012-08-17'], 'testing': ['2012-08-20', '2012-09-20', '2012-08-20', '2013-08-20']},\n",
    "          {'training': ['2007-08-17', '2015-05-19'], 'testing': ['2015-08-19', '2015-09-21', '2015-05-19', '2016-04-20']}]\n",
    "#\n",
    "#         'Dates 3': [\n",
    "#          {'training': ['1998-08-19', '2000-08-18'], 'testing': ['2000-08-21', '2000-09-20', '2000-08-21', '2001-08-20']},\n",
    "#          {'training': ['2000-08-18', '2002-08-19'], 'testing': ['2002-08-20', '2002-09-20', '2002-08-20', '2003-08-20']},\n",
    "#          {'training': ['2002-08-19', '2004-08-19'], 'testing': ['2004-08-20', '2004-09-20', '2004-08-20', '2005-08-19']},\n",
    "#          {'training': ['2004-08-19', '2006-08-18'], 'testing': ['2006-08-21', '2006-09-20', '2006-08-21', '2007-08-20']},\n",
    "#          {'training': ['2006-08-17', '2008-08-19'], 'testing': ['2008-08-20', '2008-09-22', '2008-08-20', '2009-08-20']},\n",
    "#          {'training': ['2008-08-19', '2010-08-19'], 'testing': ['2010-08-19', '2010-09-20', '2010-08-19', '2011-08-19']},\n",
    "#          {'training': ['2010-08-19', '2012-08-17'], 'testing': ['2012-08-20', '2012-09-20', '2012-08-20', '2013-08-20']},\n",
    "#          {'training': ['2012-08-17', '2015-05-19'], 'testing': ['2015-08-20', '2015-09-21', '2015-05-19', '2016-04-20']}]        \n",
    "        }\n",
    "    \n",
    "colY = 'IBEX_RD_B1_Close'\n",
    "\n",
    "features_list = []\n",
    "num_experiments = 0\n",
    "\n",
    "#df_y = dataset[colY]\n",
    "df_y = dataset[colY].shift(-1)\n",
    "last_row = df_y.shape[0]-1\n",
    "df_y = df_y.drop(last_row, axis=0)\n",
    "\n",
    "keys_dates = dates.keys()\n",
    "keys_dates.sort()\n",
    "\n",
    "document = Document()\n",
    "document.add_heading('Experimentos', 0)\n",
    "\n",
    "print >> log, 'Dataset shape %s' % str(dataset.shape)\n",
    "print >> log, 'Y shape %s' % str(df_y.shape)\n",
    "\n",
    "start = time.time()\n",
    "          \n",
    "for key_d in keys_dates:\n",
    "    date_list = dates[key_d]\n",
    "    document.add_heading(str(key_d), level=1)\n",
    "    print \"==================================================\"\n",
    "    print \"                    %s                     \" % key_d \n",
    "    print \"==================================================\"\n",
    "    print >> log, \"==================================================\"\n",
    "    print >> log, \"                    %s                     \" % str(key_d)\n",
    "    print >> log, \"==================================================\"\n",
    "    ##Trainig testing arrays\n",
    "    df_x = dataset\n",
    "    df_x = df_x.drop(last_row, axis=0)\n",
    "\n",
    "    ## Table headings\n",
    "    table = document.add_table(rows=1, cols=len(algorithms_list))\n",
    "    hdr_cells = table.rows[0].cells\n",
    "    for i in range(len(algorithms_list)):\n",
    "        hdr_cells[i].text = algorithms_list[i]\n",
    "    \n",
    "    df_x_no_date = df_x.drop('Date', axis=1)\n",
    "    x_np = np.nan_to_num(np.asarray(df_x_no_date))\n",
    "    x_np, variance = features_analysis.pca_analysis(x_np, 50)\n",
    "    y_np = np.nan_to_num(np.asarray(df_y))    \n",
    "\n",
    "    iteration_dates = 1\n",
    "\n",
    "    for iteration in date_list:\n",
    "        print \"--------------------------------------------------\"\n",
    "        print \"                   Iteration %s           \" % str(iteration_dates) \n",
    "        print \"--------------------------------------------------\"                \n",
    "        print >> log, \"--------------------------------------------------\"                \n",
    "        print >> log, \"                   Iteration %s            \" % str(iteration_dates)\n",
    "        print >> log, \"--------------------------------------------------\"                \n",
    "        training_list = iteration['training']\n",
    "        testing_list = iteration['testing']\n",
    "        current_tables = len(testing_list)/2\n",
    "        iteration_dates += 1\n",
    "        \n",
    "        # Stacking init            \n",
    "        clf = Stacking.Stacking(models, stack=False, fwls=False, model_selection=False, log=log)\n",
    "        \n",
    "        for index_test in range(0,len(testing_list),2):\n",
    "            print \"..................................................\"                    \n",
    "            print \"                 Testing array %s           \" % str(index_test) \n",
    "            print \"..................................................\"\n",
    "            print >> log, \"..................................................\"\n",
    "            print >> log, \"                 Testing array %s            \" % str(index_test)\n",
    "            print >> log, \"..................................................\"\n",
    "\n",
    "            ## Training and testing indices\n",
    "            training_dates = Iteration.Iteration(training_list[0], training_list[1])\n",
    "            testing_dates  = Iteration.Iteration(testing_list[index_test+0], testing_list[index_test+1])\n",
    "            training_dates.calculate_indices(dataset)\n",
    "            testing_dates.calculate_indices(dataset)#\n",
    "\n",
    "            trainDates = []\n",
    "            testDates = []\n",
    "            trainDates.append(training_dates.lowerIndex)\n",
    "            trainDates.append(training_dates.upperIndex)\n",
    "            testDates.append(testing_dates.lowerIndex)\n",
    "            testDates.append(testing_dates.upperIndex)#\n",
    "\n",
    "            total = (trainDates[1]-trainDates[0]) + (testDates[1]-testDates[0])\n",
    "            tr = float(trainDates[1]-trainDates[0]) / total * 100.0\n",
    "            te = float(testDates[1]-testDates[0]) / total * 100.0\n",
    "    \n",
    "            print >> log, \"Training: from %s to %s\" % (str(training_dates.startDate), str(training_dates.endDate))\n",
    "            print >> log, \"Testing: from %s to %s\" % (str(testing_dates.startDate), str(testing_dates.endDate))\n",
    "            print >> log, \"%.3f %% training %.3f %% testing\" % (tr,te)\n",
    "            print >> log, \"%d training %d testing\" % (trainDates[1]-trainDates[0], testDates[1]-testDates[0])   \n",
    "\n",
    "            trainX_raw, trainY, testX_raw, testY = ml_dataset.np_train_arrays_experiments(x_np, y_np, trainDates, testDates)\n",
    "            print trainX.shape\n",
    "            print trainY.shape\n",
    "            print testX.shape\n",
    "            print testY.shape\n",
    "            \n",
    "            ## Fit stacking model\n",
    "            if index_test == 0:\n",
    "                clf.fit(trainY, trainX)\n",
    "                 \n",
    "                \n",
    "            ###  Metrics\n",
    "            print >> log, \"computing cv score\"\n",
    "            mean_auc = 0.0\n",
    "            mean_accuracy = 0.0\n",
    "            iter_ = 1\n",
    " \n",
    "            cv_preds, models_score, models_f1 = clf.predict(trainY, trainX, testX, testY, show_steps=True)\n",
    "            cv_preds_bin = np.round_(cv_preds, decimals=0)\n",
    "            accuracy = metrics.accuracy_score(testY, cv_preds_bin)\n",
    "            f1 = metrics.f1_score(testY, cv_preds_bin)\n",
    "            print >> log, \"Accuracy: %.2f\" % accuracy\n",
    "            \n",
    "            ##  header\n",
    "            row_cells = table.add_row().cells\n",
    "    \n",
    "            row_cells[0].text = key_d + '.'+ str(iteration) + '.'+ str(index_test)\n",
    "            col = 1\n",
    "            print models_score\n",
    "            ##Table test X_X row\n",
    "            for model in range(len(models_score)):\n",
    "                cell = (\"%.2f%%\\n\" % (models_score[model]*100))\n",
    "                row_cells[col].text = cell\n",
    "                col += 1\n",
    " \n",
    " \n",
    "                \n",
    "            num_experiments += 1\n",
    "                    \n",
    "            \n",
    "\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "\n",
    "document.save('experiments.docx')\n",
    "log.close()\n",
    "print \"DONE. %s experiments\" % str(num_experiments)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainX_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data_manipulation.write_csv_data(GOLD, '/Users/Pablo/Desktop/TFM/Data/GOLD.csv')\n",
    "#data_manipulation.write_csv_data(SILVER, '/Users/Pablo/Desktop/TFM/Data/SILVER.csv')\n",
    "#data_manipulation.write_csv_data(PLAT, '/Users/Pablo/Desktop/TFM/Data/PLAT.csv')\n",
    "#data_manipulation.write_csv_data(OIL_BRENT, '/Users/Pablo/Desktop/TFM/Data/OIL_BRENT.csv')\n",
    "#\n",
    "#data_manipulation.write_csv_data(USD_GBP, '/Users/Pablo/Desktop/TFM/Data/USD_GBP.csv')\n",
    "#data_manipulation.write_csv_data(JPY_USD, '/Users/Pablo/Desktop/TFM/Data/JPY_USD.csv')\n",
    "#data_manipulation.write_csv_data(AUD_USD, '/Users/Pablo/Desktop/TFM/Data/AUD_USD.csv')\n",
    "#\n",
    "#data_manipulation.write_csv_data(INDEX_DJIA, '/Users/Pablo/Desktop/TFM/Data/INDEX_DJIA.csv')\n",
    "#data_manipulation.write_csv_data(INDEX_HSI, '/Users/Pablo/Desktop/TFM/Data/INDEX_HSI.csv')\n",
    "#data_manipulation.write_csv_data(INDEX_IBEX, '/Users/Pablo/Desktop/TFM/Data/INDEX_IBEX.csv')\n",
    "#data_manipulation.write_csv_data(INDEX_N225, '/Users/Pablo/Desktop/TFM/Data/INDEX_N225.csv')\n",
    "#data_manipulation.write_csv_data(INDEX_SP500, '/Users/Pablo/Desktop/TFM/Data/INDEX_SP500.csv')\n",
    "#data_manipulation.write_csv_data(INDEX_AXJO, '/Users/Pablo/Desktop/TFM/Data/INDEX_AXJO.csv')\n",
    "#data_manipulation.write_csv_data(INDEX_FCHI, '/Users/Pablo/Desktop/TFM/Data/INDEX_FCHI.csv')\n",
    "#data_manipulation.write_csv_data(INDEX_GDAXI, '/Users/Pablo/Desktop/TFM/Data/INDEX_GDAXI.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "import six\n",
    "import Quandl\n",
    "import calendar\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import pylab as pylab\n",
    "from datetime import datetime\n",
    "\n",
    "from matplotlib import colors\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "import statsmodels.stats.stattools as stats_stattools\n",
    "import statsmodels.tsa.stattools as tsa_stattools\n",
    "import statsmodels.tsa.seasonal as tsa_seasonal\n",
    "import statsmodels.api as sm \n",
    "import xgboost as xgb\n",
    "from unbalanced_dataset import SMOTE\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn import metrics, cross_validation, linear_model, ensemble\n",
    "from sklearn import feature_selection\n",
    "from sklearn import decomposition\n",
    "from sklearn import discriminant_analysis\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import sys\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from helpers import features_analysis, procces_stocks, data_manipulation, download_quandl_data, ml_dataset\n",
    "from classes import Iteration, Stacking, Boosting\n",
    "\n",
    "fig_size = [10, 6]\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "sb.set_style('darkgrid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "GOLD = data_manipulation.read_csv_data('/Users/Pablo/Desktop/TFM/Data/GOLD.csv')\n",
    "SILVER = data_manipulation.read_csv_data('/Users/Pablo/Desktop/TFM/Data/SILVER.csv')\n",
    "PLAT = data_manipulation.read_csv_data('/Users/Pablo/Desktop/TFM/Data/PLAT.csv')\n",
    "OIL_BRENT = data_manipulation.read_csv_data('/Users/Pablo/Desktop/TFM/Data/OIL_BRENT.csv')\n",
    "INDEX_DJIA = data_manipulation.read_csv_data('/Users/Pablo/Desktop/TFM/Data/INDEX_DJIA.csv')\n",
    "INDEX_HSI = data_manipulation.read_csv_data('/Users/Pablo/Desktop/TFM/Data/INDEX_HSI.csv')\n",
    "INDEX_IBEX = data_manipulation.read_csv_data('/Users/Pablo/Desktop/TFM/Data/INDEX_IBEX.csv')\n",
    "INDEX_N225 = data_manipulation.read_csv_data('/Users/Pablo/Desktop/TFM/Data/INDEX_N225.csv')\n",
    "INDEX_SP500 = data_manipulation.read_csv_data('/Users/Pablo/Desktop/TFM/Data/INDEX_SP500.csv')\n",
    "\n",
    "values_names = ['GOLD', 'SILVER', 'PLAT', 'OIL_BRENT', 'DJIA', 'HSI', 'IBEX', 'N225', 'SP500']\n",
    "values_dfs = [GOLD, SILVER, PLAT, OIL_BRENT, INDEX_DJIA, INDEX_HSI, INDEX_IBEX, INDEX_N225, INDEX_SP500]\n",
    "values_cols = ['USD_AM', 'USD', 'USD_AM', 'USD', 'Open', 'Open', 'Open', 'Open', 'Open']\n",
    "dict_dfs_cols = {}\n",
    "\n",
    "for index in range(len(values_names)):\n",
    "    name = values_names[index]\n",
    "    df = values_dfs[index]    \n",
    "    cols = df.columns.values\n",
    "    new_cols = [x for x in cols if x not in ['Date']]\n",
    "    #new_cols = [x for x in cols if x not in ['Date', 'USD_AM', 'USD_PM', 'USD', 'Open', 'High', 'Low', 'Close', 'Volume', 'Adjusted Close']]    \n",
    "    \n",
    "    dict_dfs_cols[name] = new_cols\n",
    "\n",
    "dataset = ml_dataset.generate_df_dataset(values_names, values_dfs, dict_dfs_cols)\n",
    "\n",
    "#First 30 row\n",
    "dataset = dataset[31:]\n",
    "dataset = dataset.reset_index(drop=True)\n",
    "datasetY = dataset.copy(deep=True)\n",
    "#dataset = dataset.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_columns_from_dataset(dataset, predicting='close', shifted = False):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    ###########################\n",
    "    # predicting close price: #\n",
    "    ###########################        \n",
    "    colsToRemove = []\n",
    "    colsToShift = []\n",
    "    \n",
    "    if predicting == 'close':\n",
    "        if not shifted:        \n",
    "            colsToRemove.extend([col for col in dataset.columns if 'Date' in col])\n",
    "            colsToRemove.extend([col for col in dataset.columns if 'GOLD' in col and not '_AM' in col])\n",
    "            colsToRemove.extend([col for col in dataset.columns if 'SILVER' in col and not '_USD' in col])\n",
    "            colsToRemove.extend([col for col in dataset.columns if 'OIL_BRENT' in col and not '_USD' in col])\n",
    "            colsToRemove.extend([col for col in dataset.columns if 'PLAT' in col and not '_AM' in col])\n",
    "            colsToRemove.extend([col for col in dataset.columns if 'DJIA' in col and not '_Open' in col])\n",
    "            colsToRemove.extend([col for col in dataset.columns if 'HSI' in col and '_Date' in col])\n",
    "            colsToRemove.extend([col for col in dataset.columns if 'IBEX' in col and not '_Open' in col])\n",
    "            colsToRemove.extend([col for col in dataset.columns if 'N225' in col and'_Date' in col])\n",
    "            colsToRemove.extend([col for col in dataset.columns if 'SP500' in col and not '_Open' in col])\n",
    "            colsToRemove.remove('IBEX_RD_B1_Close')            \n",
    "        else:\n",
    "            colsToRemove.extend([col for col in dataset.columns if 'Date' in col])            \n",
    "            colsToShift.extend([col for col in dataset.columns if 'GOLD' in col and not '_AM' in col])\n",
    "            colsToShift.extend([col for col in dataset.columns if 'SILVER' in col and not '_USD' in col])\n",
    "            colsToShift.extend([col for col in dataset.columns if 'OIL_BRENT' in col and not '_USD' in col])\n",
    "            colsToShift.extend([col for col in dataset.columns if 'PLAT' in col and not '_AM' in col])\n",
    "            colsToShift.extend([col for col in dataset.columns if 'DJIA' in col and not '_Open' in col])\n",
    "            colsToShift.extend([col for col in dataset.columns if 'HSI' in col and '_Date' in col])\n",
    "            colsToShift.extend([col for col in dataset.columns if 'IBEX' in col and not '_Open' in col])\n",
    "            colsToShift.extend([col for col in dataset.columns if 'N225' in col and'_Date' in col])\n",
    "            colsToShift.extend([col for col in dataset.columns if 'SP500' in col and not '_Open' in col])\n",
    "            colsToShift.remove('IBEX_RD_B1_Close')  \n",
    "\n",
    "    ###########################\n",
    "    # predicting open price: #\n",
    "    ###########################\n",
    "\n",
    "    if predicting == 'open' and not shifted:            \n",
    "            colsToRemove.extend([col for col in dataset.columns if 'Date' in col])\n",
    "            colsToRemove.extend([col for col in dataset.columns if 'GOLD' in col])\n",
    "            colsToRemove.extend([col for col in dataset.columns if 'SILVER' in col])\n",
    "            colsToRemove.extend([col for col in dataset.columns if 'PLAT' in col])\n",
    "            colsToRemove.extend([col for col in dataset.columns if 'OIL_BRENT' in col])    \n",
    "            colsToRemove.extend([col for col in dataset.columns if 'DJIA' in col])\n",
    "            colsToRemove.extend([col for col in dataset.columns if 'HSI' in col and '_Open' in col])\n",
    "            colsToRemove.extend([col for col in dataset.columns if 'IBEX' in col])\n",
    "            colsToRemove.extend([col for col in dataset.columns if 'N225' in col and'_Date' in col])\n",
    "            colsToRemove.extend([col for col in dataset.columns if 'SP500' in col])\n",
    "            colsToRemove.remove('IBEX_RD_B1_Open')\n",
    "\n",
    "    colsToShift = list(set(colsToShift) - set(colsToRemove)) \n",
    "    df = dataset.drop(colsToRemove, axis = 1)\n",
    "    if shifted: \n",
    "        df[colsToShift] = df[colsToShift].shift(1)\n",
    "        df = df[1:]\n",
    "        df = df.reset_index(drop=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def dataset_to_train_using_dates(dataset, trainDates, testDates, predicting = 'close', binary = False, shiftFeatures = False, shiftTarget = False):\n",
    "    \"\"\"\n",
    "    \n",
    "    Parameter\n",
    "    ---------------------\n",
    "    - dataset: dataframe containing all available columns for a set of dates\n",
    "    - trainDates: list containing the start training day and end training day\n",
    "    - testDates: list containing the start training day and end testing day\n",
    "    - namesToRemove: partial names to search to remove those columns\n",
    "    - colY: name of target\n",
    "    - binary: boolean detemines whether features will be binary only \n",
    "    - shifted: boolean detemines whether rows will be shifted by one\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    if shiftFeatures==True and shiftTarget==True:\n",
    "        raise ValueError(\"Features and Target cannot be shifted at the same time\")\n",
    "        \n",
    "    dataset = remove_columns_from_dataset(dataset, predicting = predicting, shifted = shiftFeatures)\n",
    "\n",
    "\n",
    "    ###########################\n",
    "    # predicting close price: #\n",
    "    ###########################    \n",
    "    colY = ''\n",
    "    colsToRemove = []\n",
    "\n",
    "    if predicting == 'close': \n",
    "        colY = 'IBEX_RD_B1_Close'\n",
    "        colsToRemove = ['IBEX_RD_B1_Close']\n",
    "    if predicting == 'open': \n",
    "        colY = 'IBEX_RD_B1_Open'\n",
    "        colsToRemove = ['IBEX_RD_B1_Close', 'IBEX_RD_B1_Open']\n",
    "\n",
    "    train_df = dataset.iloc[trainDates[0]:trainDates[1]+1,]    \n",
    "    test_df = dataset.iloc[testDates[0]:testDates[1]+1,]\n",
    "        \n",
    "    if binary:        \n",
    "        colsToRemove.extend([col for col in dataset.columns if '_B' not in col])\n",
    "        colsToRemove = list(set(colsToRemove))\n",
    "        trainX = np.nan_to_num(np.asarray(train_df.drop(colsToRemove, axis = 1)))\n",
    "        testX = np.nan_to_num(np.asarray(test_df.drop(colsToRemove, axis = 1)))        \n",
    "    else:        \n",
    "        colsToRemove = list(set(colsToRemove))\n",
    "        trainX = np.nan_to_num(np.asarray(train_df.drop(colsToRemove, axis = 1)))\n",
    "        testX = np.nan_to_num(np.asarray(test_df.drop(colsToRemove, axis = 1)))\n",
    "\n",
    "    if shiftTarget:        \n",
    "        trainY = np.nan_to_num(np.asarray(train_df[colY].shift(1)))[:]\n",
    "        testY = np.nan_to_num(np.asarray(test_df[colY].shift(1)))[:]\n",
    "        trainX = trainX[1:]\n",
    "        testX = testX[1:]\n",
    "    else:\n",
    "        if shiftFeatures:\n",
    "            trainY = np.nan_to_num(np.asarray(train_df[colY].shift(-1)))\n",
    "            testY = np.nan_to_num(np.asarray(test_df[colY].shift(-1)))\n",
    "            trainX = trainX[1:-1,1:-1]\n",
    "            trainY = trainY[1:-1]\n",
    "            testX = testX[1:-1,1:-1]\n",
    "            testY = testY[1:-1]\n",
    "        else:\n",
    "            trainY = np.nan_to_num(np.asarray(train_df[colY]))\n",
    "            testY = np.nan_to_num(np.asarray(test_df[colY]))\n",
    "\n",
    "    #df = df.drop(dataset.index[-1,], axis=0)\n",
    "\n",
    "    columns_names = dataset.drop(colsToRemove, axis=1).columns.values\n",
    "\n",
    "    return trainX, trainY, testX, testY, columns_names, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_dates = Iteration.Iteration('1993-08-19', '2012-07-06')\n",
    "testing_dates  = Iteration.Iteration('2012-07-09', '2016-04-20')\n",
    "training_dates.calculate_indices(dataset)\n",
    "testing_dates.calculate_indices(dataset)\n",
    "\n",
    "trainDates = []\n",
    "testDates = []\n",
    "trainDates.append(training_dates.lowerIndex)\n",
    "trainDates.append(training_dates.upperIndex)\n",
    "testDates.append(testing_dates.lowerIndex)\n",
    "testDates.append(testing_dates.upperIndex)\n",
    "\n",
    "trainX, trainY, testX, testY, cols, new_dataset = dataset_to_train_using_dates(dataset, trainDates, testDates, binary=False, shiftFeatures=False, shiftTarget=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = ensemble.ExtraTreesClassifier(random_state=1729, n_estimators=250, n_jobs=-1)\n",
    "selector = clf.fit(trainX, trainY)\n",
    "\n",
    "fs = feature_selection.SelectFromModel(selector, prefit=True)\n",
    "trainX = fs.transform(trainX)\n",
    "testX = fs.transform(testX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainX, testX = ml_dataset.feature_selection_trees(trainX, trainY, testX, testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4757, 112)\n",
      "(4757,)\n",
      "(952, 112)\n",
      "(952,)\n",
      "[ True False False ...,  True False False]\n"
     ]
    }
   ],
   "source": [
    "print trainX.shape\n",
    "print trainY.shape\n",
    "print testX.shape\n",
    "print testY.shape\n",
    "print trainY.astype('bool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> AUC: 0.5760 \n",
      "> Score: 0.5777 \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.55      0.55       439\n",
      "          1       0.61      0.60      0.60       513\n",
      "\n",
      "avg / total       0.58      0.58      0.58       952\n",
      "\n",
      "[0 1 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 1 1 0 1 0 1 0\n",
      " 1 0 0 1 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 1 0 0\n",
      " 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 1 1\n",
      " 0 1 1 0 0 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1\n",
      " 1 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 1 0 1 1 0 0 1 1 0 0 1 1 0 1 1 1 0 1 0 1 1\n",
      " 0 1 1 1 1 0 0 0 0 0 1 1 0 1 1 0 1 0 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 0\n",
      " 0 0 1 1 1 0 0 1 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 1 0\n",
      " 1 1 0 0 1 0 0 1 1 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0\n",
      " 0 0 0 0 1 1 1 0 0 0 0 1 0 1 1 1 0 1 1 0 1 1 1 0 1 1 1 0 0 0 0 0 1 0 1 0 0\n",
      " 1 1 1 0 0 0 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 1\n",
      " 1 1 0 0 0 0 1 1 0 1 1 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0 1 1 1 1 0 0 0 1 0 0\n",
      " 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 0 1 1 0 1 1 0 1 0 1 1 1 0 0 1 0 0 1 1 0 0\n",
      " 0 1 1 1 0 1 0 0 0 1 0 1 0 0 0 0 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0 1 1 1 0 1 1\n",
      " 1 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0\n",
      " 0 1 0 0 0 0 0 1 1 1 1 1 0 1 0 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 0\n",
      " 0 0 1 0 0 0 1 0 1 1 0 0 1 0 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 0 1 0\n",
      " 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1 0 0 1\n",
      " 1 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0\n",
      " 0 0 1 1 0 0 1 1 0 1 0 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1\n",
      " 1 1 1 0 0 1 0 1 0 1 1 0 1 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1 0 0 1 1 0 0 1\n",
      " 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 1 1 1 1 0 0 0 0 1 1\n",
      " 0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 0 1 0 0 1 1\n",
      " 1 1 0 0 0 0 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 0 0 0 0 0 1 0 0 1 1 1 1 1\n",
      " 1 1 0 1 1 0 0 0 1 0 0 0 0 0 0 1 1 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1\n",
      " 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 0 1 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "#svc = svm.SVC()\n",
    "#svc.fit(trainX, trainY)\n",
    "svc = ensemble.RandomForestClassifier(n_estimators=200)\n",
    "svc.fit(trainX, trainY)\n",
    "\n",
    "fpr, tpr, _ = metrics.roc_curve(testY, svc.predict(testX))\n",
    "print \"> AUC: %.4f \" % metrics.auc(fpr, tpr)\n",
    "print \"> Score: %.4f \" % svc.score(testX, testY)\n",
    "print metrics.classification_report(testY, svc.predict(testX))\n",
    "print svc.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data_manipulation.write_csv_data(GOLD_new2, '/Users/Pablo/Desktop/TFM/Data/GOLD.csv')\n",
    "#data_manipulation.write_csv_data(SILVER_new2, '/Users/Pablo/Desktop/TFM/Data/SILVER.csv')\n",
    "#data_manipulation.write_csv_data(PLAT_new2, '/Users/Pablo/Desktop/TFM/Data/PLAT.csv')\n",
    "#data_manipulation.write_csv_data(OIL_BRENT_new2, '/Users/Pablo/Desktop/TFM/Data/OIL_BRENT.csv')\n",
    "#data_manipulation.write_csv_data(INDEX_DJIA_new2, '/Users/Pablo/Desktop/TFM/Data/INDEX_DJIA.csv')\n",
    "#data_manipulation.write_csv_data(INDEX_HSI_new2, '/Users/Pablo/Desktop/TFM/Data/INDEX_HSI.csv')\n",
    "#data_manipulation.write_csv_data(INDEX_IBEX_new2, '/Users/Pablo/Desktop/TFM/Data/INDEX_IBEX.csv')\n",
    "#data_manipulation.write_csv_data(INDEX_N225_new2, '/Users/Pablo/Desktop/TFM/Data/INDEX_N225.csv')\n",
    "#data_manipulation.write_csv_data(INDEX_SP500_new2, '/Users/Pablo/Desktop/TFM/Data/INDEX_SP500.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
